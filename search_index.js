var documenterSearchIndex = {"docs":
[{"location":"intro/#Modeling-Chua's-Circuit-using-a-Neural-Ordinary-Differential-Equation","page":"Introduction","title":"Modeling Chua's Circuit using a Neural Ordinary Differential Equation","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"April Herwig","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"March 2023","category":"page"},{"location":"intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Chua's circuit is a standard case study in the literature on chaotic dynamics [9]. The simplicity of the mathematical model, a non-stiff three-dimensional system of ordinary differential equations, makes it an attractive example for many numerical experiments. The goal of this project is to train a neural ordinary differential equation using trajectory data, and determine a (heuristic) minimal amount of data required to obtain a satisfying model. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Chua's circuit was initially developed in 1986 with the goal (among others) to determine whether chaotic dynamics can appear in real-life continuous dynamical systems, or if chaos could only exist in mathematical abstraction [4]. Indeed, a strange attractor observed in experiment [10], matching the one observed through numerical simulation. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"A schematic diagram of the simplest canonical Chua's circuit is given in the below figure [2]. It consists of four components connected in parallel: two capacitors (C_1 and C_2) with a resistor R connected between them, as well as an inductor L and a nonlinear negative resistance component N_R called a Chua diode [8]. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: Circuit diagram)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Using Kirchoff's laws one derives a set of equations for the three energy storing components. After nondimensionalization the system can be written in a (simplified) canonical form [5]","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"begincases\n    dotx = alpha left( y - m_0 x - m_1 f(x) right) \n    doty = x - y + z \n    dotz = - beta y  \nendcases","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"where alpha beta m_0 m_1 are real paramters and f is a nonlinear function with fixed point 0. Note that aside from f, the system is entirely linear and hence will not exhibit chaotic behavior. The original Chua oscillator used a piecewise linear function representing the diode's resistance graph, shown below [3]. However, many simpler (and smoother) functions still exhibit chaotic dynamics while being more suitable for convergence proofs of numerical algorithms. This project will use a cubic nonlinearity f(x) = frac13 x^3 with parameters alpha = 18 beta = 33 m_0 = -02 m_1 = 001.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: Voltage-Current Graph)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"The Chua oscillator exhibits a strange attractor referred to as the double scroll, as well as a surrounding periodic orbit. These can be initially supposed by observing long trajectories beginning around the two unstable fixed points x_pm = left( pm sqrt-3 m_0  m_1 0 mp sqrt-3 m_0  m_1 right)^T. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"using CairoMakie","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"# for plotting phase portraits\nfunction chua_axis(; aspect=(1,1.2,1), azimuth=pi/10)\n    fig = Figure()\n    ax = Axis3(fig[1,1], aspect=aspect, azimuth=azimuth)\n    fig, ax\nend","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"using StaticArrays, OrdinaryDiffEq","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"# Chua's circuit\nfunction v(u, p, t)\n    x, y, z = u\n    a, b, m0, m1 = p\n    SA{Float32}[ a*(y-m0*x-m1/3.0*x^3), x-y+z, -b*y ]\nend\n\n# parameters\np_ode = SA{Float32}[ 18.0, 33.0, -0.2, 0.01 ]\na, b, m0, m1 = p_ode\n\nv(u) = v(u, p_ode, 0f0)\n\n# equilibrium\nx₊ = SA{Float32}[ sqrt(-3*m0/m1), 0, -sqrt(-3*m0/m1) ]\nx₋ = -x₊\n\n# integration time\nt0, t1 = 0f0, 40f0\ntspan = (t0, t1)\ndt = 1f-2","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"x0 = SA{Float32}[2, 1.5, 6]\nprob = ODEProblem(v, x0, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fig, ax = chua_axis()\nscatter!(ax, [x₊, SA[0f0,0f0,0f0], x₋], color=:red, label=\"Equillibria\")\nlines!(ax, sol.u, label=\"Inner Attractor\")\nLegend(fig[1,2], ax)\n\nsave(\"inner.png\", fig); nothing # hide","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"x0 = SA{Float32}[2, 1.5, 6]\nprob = ODEProblem(v, x0, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt)\n\nfig, ax = chua_axis()\nscatter!(ax, [x₊, SA[0f0,0f0,0f0], x₋], color=:red, label=\"Equillibria\")\nlines!(ax, sol.u, label=\"Inner Attractor\")\n\n# only slightly different initial condition\nx0 = SA{Float32}[2, 1.5, 8]\nprob = ODEProblem(v, x0, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt)\n\nlines!(ax, sol.u, color=:green, label=\"Outer Attractor\")\nLegend(fig[1,2], ax)\n\nsave(\"outer.png\", fig); nothing # hide","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"The existence of the attractor can be numerically proven using a subdivision algorithm from the Julia package GAIO.jl [7], which provides an outer approximation of the unstable manifolds of x_pm as well as the chain recurrent set. ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"using GAIO","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"# domain\nc, r = (0,0,0), (20,5,20)\nQ = GAIO.Box(c, r)\n\n# 120 x 120 x 120 partition of the domain\nP = BoxPartition(Q, (120,120,120))\nS = cover(P, [x₊, x₋])\n\n# short trajectory (20 steps) to fully cover unstable manifold\nf_short(u) = rk4_flow_map(v, u, dt, 20)\nF_short = BoxMap(:grid, f_short, Q)\n\nW = unstable_set(F_short, S)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fig, ax = chua_axis(azimuth=3*pi/10)\nplot!(ax, W, color=(:blue, 0.5))\n\nsave(\"unstable.png\", fig); nothing # hide","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"# extend the domain\nc, r = (0,0,0), (20,20,120)\nQ = GAIO.Box(c, r)\n\n# 1 x  x 1 partition of the domain\nP = BoxPartition(Q, (1,1,1))\nS = cover(P, :)\n\n# long trajectory to avoid covering \"transitional\" set\nf_long(u) = rk4_flow_map(v, u, dt, 100)\nF_long = BoxMap(:grid, f_long, Q)\n\n# 30 subdivision steps to cover the chain recurrent set\nC = chain_recurrent_set(F_long, S, steps=24)\n\n# remove the unstable manifold which we already found\nP = C.partition\nW = cover(P, W)\nC = setdiff!(C, W)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fig, ax = chua_axis(azimuth=pi/5)\nplot!(ax, W, color=(:blue, 0.6))\nplot!(ax, C, color=(:green, 0.6))\n\nsave(\"rec.png\", fig); nothing # hide","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"# full interesting set\nA = C ∪ W\n\n# short trajectory for computing FTLE field\nσ = finite_time_lyapunov_exponents(F_short, A, T=20*dt)","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fig, ax = chua_axis()\nms = plot!(ax, σ, colormap=(:jet, 0.6))\nColorbar(fig[1,2], ms)\n\nsave(\"ftle.png\", fig); nothing # hide","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"We can conclude that the most chaotic behavior occurs for points lying on the \"transition\" between to two scrolls of the attractor. We will train two models on long trajectories with different starting values: ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"along the unstable manifold,\napproaching the periodic orbit.","category":"page"},{"location":"discussion/#Discussion","page":"Discussion","title":"Discussion","text":"","category":"section"},{"location":"discussion/#Hyperparameter-Optimization","page":"Discussion","title":"Hyperparameter Optimization","text":"","category":"section"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The general model used, as described in the model section, is summarized as follows.","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"A residual neural network with N_hidden_layers and N_weights per layer was trained using N_epochs with the RMSProp [SOURCE] gradient-based optimization method with learning rate η. The loss function from the model section with added regularization hyperparameter θ and exponential weight factor β was used:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"L(mathbfx mathbfhatx mathbfp) = left( sum_i=1^n beta^i cdot  mathbfx(t_i) - mathbfhatx(t_i mathbfp)  ^2 right) + fractheta2  mathbfp  ^2  ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The regularization factor was implemented using Flux.jl's built in OptimiserChain and  WeightDecay functions. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The hyperparameters were optimized over the following grid in the script file runscript.jl. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"weights = [10, 12, 15, 18, 20]\nhidden_layers = [3]#[1, 2, 3]\nepochs = [120]\ntfins = Float32[15]#Float32[5, 8, 10, 12, 15, 18, 20]\nβs = Float32[0.95, 0.98, 0.99, 1.]\nθs = Float32[1f-2, 1f-3, 1f-4]\nηs = Float32[1f-3]\n\nparams = [\n    (w, l, e, t, β, θ, η) for\n    w in weights,\n    l in hidden_layers,\n    e in epochs,\n    t in tfins,\n    β in βs,\n    θ in θs,\n    η in ηs\n]","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"For each hyperparameter configuration, the model was optimized and the resulting loss was recorded in an array losses. This, along with the optimized neural net parameters p for each hyperparameter configuration, was saved as a BSON file. After the grid search was completed, the optimal hyperparameter set was observed to be N_weights = 15, N_hidden_layers = 3, β = 0.99, θ = 0.0001. ","category":"page"},{"location":"discussion/#Amount-of-Data-Required","page":"Discussion","title":"Amount of Data Required","text":"","category":"section"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"We wish to observe the minimal amount of data required to obtain a satisfying approximation of the qualitative dynamics. For this, the same initial point x0 was used with different integration lengths tfin. Ideally, a hyperparameter optimization would be performed for each of the datasets. Due to hardware and time constraints, the same optimal hyperparameters from the initial search were used for all datasets. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"All trained models were evaluated on the same two trajectories starting at the initial points x_1 = ( -5 12 -4 )^T and x_2 = ( -6 12 -6 )^T converging to the double scroll attractor and outer periodic orbit, respectively. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Since the two trajectories lie in different scales, the trained models are evaluated using a relative squared error metric","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"L_rel (mathbfx mathbfhatx mathbfp) = frac1sigma^2 (mathbfx) sum_i=1^n  mathbfx(t_i) - mathbfhatx(t_i mathbfp)  ^2","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"where mathbfm(mathbfx) is the empirical mean of (mathbfx(t_i))_i = 1ldotsn and sigma^2 (mathbfx) is the empirical variance","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"sigma^2 (x) = sum_i=1^n left mathbfx(t_i) - mathbfm (mathbfx) right ^2 ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The datasets generated by the following final times were tested in the script file runscript2.jl.","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"tfins = Float32[3, 5, 8, 10, 12, 15, 18, 20]","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"using CairoMakie\nusing Random\nRandom.seed!(1234)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"# for plotting phase portraits\nfunction chua_axis(; aspect=(1,1.2,1), azimuth=pi/10, kwargs...)\n    fig = Figure()\n    ax = Axis3(fig[1,1], aspect=aspect, azimuth=azimuth, kwargs...)\n    fig, ax\nend","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"using BSON\nlosses = BSON.load( \"assets/params/losses_2023-04-02-06-43.bson\" )[:losses]","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Due to hardware constraints, a more detailed search of final times was not possible. However, even with a small dataset it was still possible to notice a trend between dataset size and evaluated loss.","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig1 = Figure()\nax1 = Axis(fig1[1,1], xlabel=\"tfin\", ylabel=\"loss\")\nms1 = lines!(ax1, tfins, map(x->losses[x], tfins))\n\nsave(\"tfinlosses.png\", fig1); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Curiously, it seems the models tranied on the most data became unstable. This can be seen when comparing longer trajectories which are in time closer to the scale of the largest Lyapunov exponent. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"We first gather the trained parameters from each dataset. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"params = Dict{Float32,Vector{Float32}}(\n    3  => BSON.load(\"assets/params/params_2023-04-02-07-05_loss_0.48747793.bson\")[:p_trained],\n    5  => BSON.load(\"assets/params/params_2023-04-02-08-16_loss_0.520202.bson\")[:p_trained],\n    8  => BSON.load(\"assets/params/params_2023-04-02-08-56_loss_0.47008443.bson\")[:p_trained],\n    10 => BSON.load(\"assets/params/params_2023-04-02-09-20_loss_0.471012.bson\")[:p_trained],\n    12 => BSON.load(\"assets/params/params_2023-04-02-09-36_loss_0.33675984.bson\")[:p_trained],\n    15 => BSON.load(\"assets/params/params_2023-04-02-09-53_loss_0.20821033.bson\")[:p_trained],\n    18 => BSON.load(\"assets/params/params_2023-04-02-10-09_loss_0.14798835.bson\")[:p_trained],\n    20 => BSON.load(\"assets/params/params_2023-04-02-10-18_loss_0.1492943.bson\")[:p_trained]\n)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"We can recreate the models using these parameters, and compare the model trajectories to the true trajectory for an unseen initial condition. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"using StaticArrays, DifferentialEquations\nusing Flux, SciMLSensitivity\nusing Statistics","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"# Chua's circuit\nfunction v(u, p, t)\n    x, y, z = u\n    a, b, m0, m1 = p\n    SA{Float32}[ a*(y-m0*x-m1/3.0*x^3), x-y+z, -b*y ]\nend\n\n# parameters\np_ode = SA{Float32}[ 18.0, 33.0, -0.2, 0.01 ]\na, b, m0, m1 = p_ode\n\nv(u) = v(u, p_ode, 0f0)\n\n# equilibrium\nx₊ = SA{Float32}[ sqrt(-3*m0/m1), 0, -sqrt(-3*m0/m1) ]\nx₋ = -x₊\n\nx1 = SA{Float32}[-5, 1.2, -4]\nx2 = SA{Float32}[-6, 1.2, -6]\n\n# integration time\nt0, t1 = 0f0, 40f0\ntspan = (t0, t1)\ndt = 1f-2","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"# the model which was trained\nnn = Chain(\n    x -> Float32.(x),\n    Dense(3 => 15, swish),\n    SkipConnection(Dense(15 => 15, swish), +),\n    SkipConnection(Dense(15 => 15, swish), +),\n    SkipConnection(Dense(15 => 15, swish), +),\n    Dense(15 => 3)\n)\n\np, re_nn = Flux.destructure(nn)\nv_neural(u, p, t) = SVector{3,Float32}(re_nn(p)(u))","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"For very long trajectories, all models will deviate from the true trajectory, due to the chaotic nature of the system. Interestingly however, the model trained until tfin = 10 was successfully able to capture the pseudoperiodic nature of the attractor. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig2 = Figure(resolution=(1200,600))\n\nax21 = Axis3(fig2[1,1], aspect=(1,1.2,1), azimuth=pi/10, title=\"True Trajectory\")\nax22 = Axis3(fig2[1,2], aspect=(1,1.2,1), azimuth=pi/10, title=\"Trained up to tfin = 3\")\nax23 = Axis3(fig2[1,3], aspect=(1,1.2,1), azimuth=pi/10, title=\"Trained up to tfin = 5\")\nax24 = Axis3(fig2[2,1], aspect=(1,1.2,1), azimuth=pi/10, title=\"Trained up to tfin = 10\")\nax25 = Axis3(fig2[2,2], aspect=(1,1.2,1), azimuth=pi/10, title=\"Trained up to tfin = 12\")\nax26 = Axis3(fig2[2,3], aspect=(1,1.2,1), azimuth=pi/10, title=\"Trained up to tfin = 20\")\n\nprob = ODEProblem(v, x1, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax21, sol.u)\n\nprob = ODEProblem(v_neural, x1, (t0, t1), params[3])\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax22, sol.u)\n\nprob = ODEProblem(v_neural, x1, (t0, t1), params[5])\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax23, sol.u)\n\nprob = ODEProblem(v_neural, x1, (t0, t1), params[10])\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax24, sol.u)\n\nprob = ODEProblem(v_neural, x1, (t0, t1), params[12])\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax25, sol.u)\n\nprob = ODEProblem(v_neural, x1, (t0, t1), params[20])\nsol = solve(prob, RK4(), saveat=dt)\nlines!(ax26, sol.u)\n\nsave(\"trajs.png\", fig2); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"t2 = 3f0\nx3 = SA{Float32}[3, 1.1, 4]\n\nfig3 = Figure(resolution=(1200,600))\n\nax31 = Axis(fig3[1,1], title=\"Trained up to tfin = 3\")\nax32 = Axis(fig3[1,2], title=\"Trained up to tfin = 8\")\nax33 = Axis(fig3[2,1], title=\"Trained up to tfin = 12\")\nax34 = Axis(fig3[2,2], title=\"Trained up to tfin = 15\")\n\nprob = ODEProblem(v, x3, (t0, t2), p_ode)\ntrue_sol = reinterpret(reshape, Float32, solve(prob, RK4(), saveat=dt).u)\n\nfor (ax, tfin) in zip([ax31, ax32, ax33, ax34], Float32[3, 8, 12, 15])\n    lines!(ax, t0:dt:t2, true_sol[1, :], color=:darkblue, linestyle=:dot, linewidth=2, label=\"True solution dim 1\")\n    lines!(ax, t0:dt:t2, true_sol[2, :], color=:darkgreen, linestyle=:dot, linewidth=2, label=\"True solution dim 2\")\n    lines!(ax, t0:dt:t2, true_sol[3, :], color=:darkred, linestyle=:dot, linewidth=2, label=\"True solution dim 3\")\n    \n    prob = ODEProblem(v_neural, x3, (t0, t2), params[tfin])\n    sol = reinterpret(reshape, Float32, solve(prob, RK4(), saveat=dt).u)\n\n    lines!(ax, t0:dt:t2, sol[1, :], color=:blue, linewidth=2, label=\"trained model dim 1\")\n    lines!(ax, t0:dt:t2, sol[2, :], color=:green, linewidth=2, label=\"trained model dim 2\")\n    lines!(ax, t0:dt:t2, sol[3, :], color=:red, linewidth=2, label=\"trained model dim 3\")\nend\n\nxlims = (t0, t2)\nylims = 1.2 .* extrema(true_sol)\nlimits!(ax31, xlims, ylims)\nlimits!(ax32, xlims, ylims)\nlimits!(ax33, xlims, ylims)\nlimits!(ax34, xlims, ylims)\n\naxislegend(ax34, position=:rb)\n\nsave(\"comps1.png\", fig3); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"It is clear that some models quickly diverge, despite having more data. There are two main reasons for this issue:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"After ca. 90 epochs, all models began \"jumping\" from the optimal objective value. This problem is caused largely by the optimization algorithm itself. A learning rate which is too large can cause the optimizer to act chaotically, so a more aggressive reduction in learning rate would be beneficial. Furthermore, switching to a local optimizer (e.g. the LBFGS quasi-Newton method) can additionally be used once a global optimizer has reached an approximate solution. Such local optimizers are built in to the Optimization.jl ecosystem. An even simpler improvement would be to store the \"best so far\" parameters and update them at each iteration step. This could be added in the training loop as follows: ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"if false # do not actually, run, just a demo implementation\n\nN_epochs = 180\nl_best = Inf32\nbest_params = model.p\n\nfor i_e = 1:N_epochs\n\n    Flux.train!(model, train, opt_state) do m, t, x\n        result = m((t,x))\n        loss(result, x)\n    end \n\n    l_best = min(l_best, l)\n    global l = mean(valid) do v\n        loss( model(v), v[2], β )\n    end\n\n    if l <= l_best\n        global best_params .= model.p\n    end\n\n    if i_e % 30 == 0\n        η /= 2\n        Flux.adjust!(opt_state, η)\n    end\n\nend\n\nend","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The model was trained on very short time intervals to avoid divergence early in training. This was necessary since the initial models can be very expansive / unstable in certain directions. As training evolves, however, the intervals no longer need to be quite as small. A simple imperovement to the trainnig process is therefore to run a second training loop (potentially with less epochs) on larger time slices. Note that this requires more training time, which due to hardware constraints was not possible. This can also be implemented quite easily:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"if false # do not actually, run, just a demo implementation\n\nN_epochs = 128\ndata_length = 8\nmax_length = 64\n\nwhile data_length <= max_length\n\n    train, valid = NODEDataloader(sol, data_length; dt=dt, valid_set=0.2, GPU=false#=true=#)\n\n    for i_e in 1:N_epochs\n        # run training loop as above\n    end\n\n    data_length *= 2\n    N_epochs /= 2\n\nend\n\nend","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"It should be noted that even the more \"well baheved\" models still exhibit very large expansion rates, as an analysis of the Lyapounov exponents shows. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"using GAIO","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"c, r = (0,0,0), (20,20,120)\nQ = GAIO.Box(c, r)\n\nP = BoxPartition(Q)\nS = cover(P, :)\n\nnn_10 = re_nn(params[10])","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"f(x) = rk4_flow_map(v, x, dt, 100)\nF = BoxMap(:grid, f, Q)\n\n# 30 subdivision steps to cover the chain recurrent set\nC = chain_recurrent_set(F, S, steps=24)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"f(x) = rk4_flow_map(v, x, dt, 20)\nF = BoxMap(:grid, f, Q)\nσ = finite_time_lyapunov_exponents(F, C, T=20*dt)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"f_neural(x) = rk4_flow_map(SVector{3,Float32} ∘ nn_10, x, dt, 20)\nF_neural = BoxMap(:grid, f_neural, Q)\nσ_neural = finite_time_lyapunov_exponents(F_neural, C, T=20*dt)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig4 = Figure(resolution=(1200,600))\ng1 = fig4[1,1] = GridLayout()\ng2 = fig4[1,2] = GridLayout()\n\nax1 = Axis3(g1[1,1], aspect=(1,1.2,1), azimuth=pi/10, title=\"True FTLE Field\")\nms1 = plot!(ax1, σ, colormap=(:jet, 0.6))\nColorbar(g1[1,2], ms1)\n\nax2 = Axis3(g2[1,1], aspect=(1,1.2,1), azimuth=pi/10, title=\"Predicted FTLE Field\")\nms2 = plot!(ax2, σ_neural, colormap=(:jet, 0.6))\nColorbar(g2[1,2], ms2)\n\nsave(\"ftles.png\", fig4); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"σ_diff = σ_neural - σ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig5 = Figure()\nax = Axis3(fig5[1,1], aspect=(1,1.2,1), azimuth=pi/10, title=\"Difference in FTLE Fields\")\nms = plot!(ax, σ_diff, colormap=(:jet, 0.6))\nColorbar(fig5[1,2], ms)\n\nsave(\"ftlecomp.png\", fig5); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/#Training-on-the-outer-Attractor","page":"Discussion","title":"Training on the outer Attractor","text":"","category":"section"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"An interesting - though not entirely unexpected - result of training the model on the outer periodic orbit was that the model was unable to capture the qualitative aspects of the inner attractor. In fact, in many cases the model sharply diverged when approaching the origin. In some sense this is a curious result, since the symmetry of the outer attractor might \"suggest\" that there is a fixed point near the origin. Knowledge of this fixed point could potentially be encoded via the method of \"explicit linearity\" mentioned in the model section. However, again due to hardware constraints, this hypothesis could not rigorously tested within an appropriate time frame. Nonetheless, the \"failure\" of the model when trained on the outer attractor leads to an array of interesting questions on how to learn \"hidden\" dynamics by choosing an appropriate neural network model based on prior knowledge. ","category":"page"},{"location":"discussion/#Perturbed-Data","page":"Discussion","title":"Perturbed Data","text":"","category":"section"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"In the final test, the data was perturbed by adding white noise to the true differential equation. There are two \"natural\" methods in which this might be done:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Integrate the true differential equation to obtain training data, then add white noise to the result. In a natural science setting, this can model using an inexact measurement device on a \"macroscopic\" system. In other words, the system can be measured (though only inaccurately) without disturbing the system state itself. An example of this type of system would be an energy balance model for large atmospheric systems. This can be implemented using DifferentialEquations.jl as follows:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"prob = ODEProblem(v, x3, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt)\n\nδ = 0.5\nsol_arr = reinterpret(reshape, Float32, sol.u)\nsol_arr .+= δ/4 * rand(Float32, size(sol_arr)) .- δ/8","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig6, ax = chua_axis()\nscatter!(ax, [x₊, SA[0f0,0f0,0f0], x₋], color=:red)\nlines!(ax, sol.u)\n\nsave(\"naive.png\", fig6); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Add a diffusion term to the differential equation itself. This way, the gathered data is a realization of a stochastic process and may not immediately capture the true nature of the system. Again in a natural science setting, this can model using a measurement device on a \"microscopic\" system. That is, the system state cannot be measured without itself being perturbed. An example of this type of system would be subatomic model of a chemical reaction. This can also be done easily using DifferentialEquations.jl","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"_noise = SA{Float32}[δ, δ, δ]\nnoise(u, p, t) = _noise\n\nprob = SDEProblem(v, noise, x₊, (t0, t1), p_ode)\nsol = solve(prob, saveat=dt)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig7, ax = chua_axis()\nscatter!(ax, [x₊, SA[0f0,0f0,0f0], x₋], color=:red)\nlines!(ax, sol.u)\n\nsave(\"stoch.png\", fig7); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Since the Chua circuit is so heavily motivated from real life considerations, the method for adding noise ought to reflect the real life as well. The addition of an oscilloscope for measuring current in a circuit does perturb the system slightly, though this change is so small that it is negligible to the cicuit's dynamics. Hence it seems reasonable to use the first approach. However, using a stochastic differential equation may be a natural extension to the project. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The model was trained as before with identical hyperparameters for the following magnitudes of the perturbation δ. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"δs = Float32[0.2, 0.3, 0.5, 0.8, 1.]","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The resulting losses were recorded. Expectedly, the loss shows a positive correlation with the magnitude of the perturbation. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"losses = BSON.load( \"assets/params/losses_2023-04-03-14-12.bson\" )[:losses]","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig8 = Figure()\nax1 = Axis(fig8[1,1], xlabel=\"δ\", ylabel=\"loss\")\nms1 = lines!(ax1, δs, map(x->losses[x], δs))\n\nsave(\"deltalosses.png\", fig8); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"params = Dict{Float32,Vector{Float32}}(\n    0.2f0  => BSON.load(\"assets/params/params_2023-04-03-17-02_loss_1.5025642.bson\")[:p_trained],\n    0.3f0  => BSON.load(\"assets/params/params_2023-04-03-17-02_loss_1.8289114.bson\")[:p_trained],\n    0.5f0  => BSON.load(\"assets/params/params_2023-04-03-16-57_loss_2.4695873.bson\")[:p_trained],\n    0.8f0  => BSON.load(\"assets/params/params_2023-04-03-16-52_loss_2.7305609.bson\")[:p_trained],\n    1.0f0  => BSON.load(\"assets/params/params_2023-04-03-15-46_loss_3.521549.bson\")[:p_trained],\n)","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"fig9 = Figure(resolution=(1200,600))\n\nax31 = Axis(fig9[1,1], title=\"Trained with δ = 0.2\")\nax32 = Axis(fig9[1,2], title=\"Trained with δ = 0.5\")\nax33 = Axis(fig9[1,3], title=\"Trained with δ = 1.0\")\n\nprob = ODEProblem(v, x3, (t0, t2), p_ode)\ntrue_sol = reinterpret(reshape, Float32, solve(prob, RK4(), saveat=dt).u)\n\nfor (ax, tfin) in zip([ax31, ax32, ax33], Float32[0.2, 0.5, 1.0])\n    lines!(ax, t0:dt:t2, true_sol[1, :], color=:darkblue, linestyle=:dot, linewidth=2, label=\"True solution dim 1\")\n    lines!(ax, t0:dt:t2, true_sol[2, :], color=:darkgreen, linestyle=:dot, linewidth=2, label=\"True solution dim 2\")\n    lines!(ax, t0:dt:t2, true_sol[3, :], color=:darkred, linestyle=:dot, linewidth=2, label=\"True solution dim 3\")\n    \n    prob = ODEProblem(v_neural, x3, (t0, t2), params[tfin])\n    sol = reinterpret(reshape, Float32, solve(prob, RK4(), saveat=dt).u)\n\n    lines!(ax, t0:dt:t2, sol[1, :], color=:blue, linewidth=2, label=\"trained model dim 1\")\n    lines!(ax, t0:dt:t2, sol[2, :], color=:green, linewidth=2, label=\"trained model dim 2\")\n    lines!(ax, t0:dt:t2, sol[3, :], color=:red, linewidth=2, label=\"trained model dim 3\")\nend\n\nxlims = (t0, t2)\nylims = 1.3 .* extrema(true_sol)\nlimits!(ax31, xlims, ylims)\nlimits!(ax32, xlims, ylims)\nlimits!(ax33, xlims, ylims)\n\naxislegend(ax33, position=:rb)\n\nsave(\"comps2.png\", fig9); nothing # hide","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"(Image: )","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"We see that increasing the magnitude of the perturbation causes the model to become more unstable. All models were able to roughly capture the period of the periodic behavior, however higher values of δ quickly diverge after just one hlalf period. Observing the lowest value of δ we see that the model is able to capture the trend but is \"conservative\" with the amplitude of the oscillations. An interesting connection to the original model is that this phenomenon was also observed when using the standard mean square error loss function in training. This might suggest that decreasing the value of the exponential weight factor \\beta may be enough to force the model into larger amplitudes. ","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"There are multiple ways in which one could improve this model:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Train on multiple perturbed trajectories. Ideally, the trajectories are very similar such that having multiple perturbations can potentially \"cancel\" each other out. This method may be difficult since the chaotic nature of the system means that it has sensitive dependence on initial conditions. Even very similar initial conditions will eventually diverge. Nonetheless simply using multiple trajectories without worrying about their \"similarity\" can already massively improve the model. \nAdd perturbations to the model itself. This could be done by adding white noise to the model integrator, i.e. construct a stochastic differential equation. Then, define the output of the model as the average over multiple realizations. This way, the magnitude of the perturbation within the model can also be trained. This may also help address the issue mentioned above, and increase the amplitude of oscillations. ","category":"page"},{"location":"discussion/#Conclusion","page":"Discussion","title":"Conclusion","text":"","category":"section"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"The complex dynamics of the Chua circuit have been widely studied since its invention. Despite its chaotic nature, a single trajectory around its \"double scroll\" attractor was enough to accurately predict the system's dynamics up to the time scale of the Lyapunov exponents. Trajectories for training were chosen along the attractor, as well as approaching an attracting periodic orbit. A neural ordinary differential equation was used to approximate the dyanmics. Multiple structures for the neural ODE were tested during development, and a residual network was deemed to be more accurate. Models were quickly able to discover the trend for trajectories, but the amplitude of oscillations were too small. To resolve this issue, a new loss function was defined, using the exponential weight factor \\beta. This significantly improved the accuracy of the model. A hyperparameter search was conducted and the optimal hyperparameters were used to analyze the amount of data required to obtain a satisfying approximation of the dynamics. The author concludes that training up to tfin = 10 - equivalently using 1000 datapoints separated by dt=0.02 - was enough to obtain such a satisfying result. Using perturbed data caused the model to perform less accuratley, though only in the sense that the model was too conservative. This is commonly a sign of underfitting. Hence, improvements to the model were suggested, and are summarized here:","category":"page"},{"location":"discussion/","page":"Discussion","title":"Discussion","text":"Reduce the learning rate adaptively and store a running best parameter set,\ntrain the model using a performant local optimizer such as LBFGS once an approximation of the optimal parameter set is found,\nslowly increase the time intervals used in training after many training iterations,\nuse multiple similar trajectories instead of only one trajectory,\nadd perturbation to the model directly if it is known that the dataset is perturbed.","category":"page"},{"location":"model/#Machine-Learning-Model","page":"Model","title":"Machine Learning Model","text":"","category":"section"},{"location":"model/","page":"Model","title":"Model","text":"We now come to the question of approximating this system based on trajectory data. The method of Chen et. al. suggests considering a recurrent neural network as a \"sequence of transformations to a hidden state mathbfh_t\" [1]:","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"mathbfh_t+1 = mathbfh_t + f(mathbfh_t theta)","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"where t = 0 ldots T mathbfh_t in mathbbR^d, and f is a single layer of a neural network. They then extend this to an ODE propagating the state of a hidden layer","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"fracd mathbfh (t)d t = f(mathbfh(t) t theta)","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"where t in 0 T, and f is now an entire neural network. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"When considering a chaotic dynamical system, the \"chaotic element\" arises due to a nonlinearity in the system. This fact is particularly present in Chua's circuit, where the model is almost entirely linear, save for one nonlinearity in the first component. Hence it is reasonable to desire that the neural network should contain a \"linear\" component, as well as a nonlinear one. Certainly there are many ways to achieve such a network; two possibilities are presented below. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"The first idea which comes to mind is to separate linear and nonlinear components explicitly in the network. More precisely, the neural network should model a function nn(x) = W cdot x + g(x) where W in mathbbR^d times d and g can be considered as an explicit nonlinearity. This can be implemented using Flux.jl's built in layers. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"using Flux","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"N_weights = 15\n\nW = Dense(3 => 3)   # dimension = 3\n\ng = Chain(\n    Dense(3 => N_weights),\n    Dense(N_weights => N_weights, swish),\n    Dense(N_weights => 3)\n)\n\nnn = Parallel(+, W, g)","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"Another option would be to build in the linearity implicitly. One could use a more \"typical\" neural network with linear input and/or output layers, and use residual network layers to compute a nonlinearity. This allows the network model to \"decide\" for itself whether the linear component is strictly necessary. A concrete implementation would look like:","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"nn = Chain(\n    Dense(3 => N_weights),\n\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n\n    Dense(N_weights => 3)\n)","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"This model presents itself as a more \"typical\" residual neural network as described in literature. However, one observes by considering the network graph that this model also permits a linear and nonlinear component. Further, this structure benefits from the design of a residual neural network: the \"vanishing gradient\" problem is reduced [6], and extra layers can be added with reduced fear of overfitting since the model can simply \"choose\" to ignore unnecessary layers. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"During development of the project, both methods were tested and the implicit form performed better. However, the difference was not large. For the remainder of this report, the implicit method will be used. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"using CairoMakie","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"# for plotting training results\nfunction plot_nde(sol, model, train; ndata=300)\n    t = sol.t[1:ndata]\n    pred = Array(model((t,train[1][2])))\n    tr = Array(sol)\n    fig, ax, ms = lines(t, pred[1, 1:ndata], label=\"Neural ODE dim 1\")\n    lines!(ax, t, pred[2, 1:ndata], label=\"Neural ODE dim 2\")\n    lines!(ax, t, pred[3, 1:ndata], label=\"Neural ODE dim 3\")\n    lines!(ax, t, tr[1, 1:ndata], label=\"Training Data dim 1\")\n    lines!(ax, t, tr[2, 1:ndata], label=\"Training Data dim 2\")\n    lines!(ax, t, tr[3, 1:ndata], label=\"Training Data dim 3\")\n    Legend(fig[1,2], ax)\n    fig, ax\nend","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"using StaticArrays, Statistics\nusing OrdinaryDiffEq, SciMLSensitivity#, CUDA\nusing NODEData, ChaoticNDETools","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"# Chua's circuit\nfunction v(u, p, t)\n    x, y, z = u\n    a, b, m0, m1 = p\n    SA{Float32}[ a*(y-m0*x-m1/3.0*x^3), x-y+z, -b*y ]\nend\n\n# parameters\np_ode = SA{Float32}[ 18.0, 33.0, -0.2, 0.01 ]\na, b, m0, m1 = p_ode\n\nv(u) = v(u, p_ode, 0f0)\n\n# equilibrium\nx₊ = SA{Float32}[ sqrt(-3*m0/m1), 0, -sqrt(-3*m0/m1) ]\nx₋ = -x₊\n\n# integration time\nt0, t1 = 0f0, 50f0\ntspan = (t0, t1)\ndt = 1f-2;","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"The data used will be the trajectories from the previous section. These are split into minibatches to both reduce the chance of the training model diverging, as well as to reduce condition number of the gadients for optimization. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"x0 = SA{Float32}[2, 1.5, 6]\nprob = ODEProblem(v, x0, (t0, t1), p_ode)\nsol = solve(prob, RK4(), saveat=dt, sensealg=InterpolatingAdjoint())","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"train, valid = NODEDataloader(sol, 8; dt=dt, valid_set=0.8, GPU=false#=true=#)\ntrain","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"The parameters of the model are extracted and flattened to a vector p so that the gradient of the loss w.r.t. p can be directly calculated. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"p, re_nn = Flux.destructure(nn)\n#p = p |> gpu\nneural_ode(u, p, t) = re_nn(p)(u)\nneural_ode(u) = neural_ode(u, p, 0f0)\n\nneural_ode_prob = ODEProblem(neural_ode, #=CuArray(x0)=#x0, tspan, p)\nmodel = ChaoticNDE(neural_ode_prob, alg=RK4(), gpu=false#=true=#, sensealg=InterpolatingAdjoint());","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"model(valid[1])","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"The final consideration required before training can be done is the loss function. The most naive loss function may be derived from the shooting method for boundary value problems. One integrates the model for some fixed time T, and compute the difference (in norm) of the model trajectory to the true trajectory data. This technique is extended analogoously to the method of multiple shooting, where the multiple small consecutive trajectories are compared. The resulting differences can be added together to obtain a scalar valued loss function, equivalent (up to a scaling factor) to a mean squared error. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"L(mathbfx mathbfhatx mathbfp) = sum_i=1^n  mathbfx(t_i) - mathbfhatx(t_i mathbfp)  ^2","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"where mathbfx mathbfhatx are true and predicted time series, evaluated at times t_1  t_2  ldots  t_n = t_1 + T. The paramteter vector p is the parameters of the neural network.  While the mean squared error works quite well, a potential downfall can occur, particularly in periodic systems. In each small trajectory, the errors of the model will compound. However, the mean squared error weighs all of the errors equally. This leads to the potential case that the model is initially incorrect, but later along the trajectory it corrects itself. The model hence learns a fundamentally wrong trajectory, and cannot easily be trained out of this error. This can be seen in the following training attempt:","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"loss(x, y) = sum(abs2, x - y)\n\nl = mean(valid) do v\n    loss( model(v), v[2] )\nend\n\nθ = 1f-4\nη = 1f-3\nopt = Flux.OptimiserChain(Flux.WeightDecay(θ), Flux.RMSProp(η))\nopt_state = Flux.setup(opt, model) \n\nN_epochs = 30\nfor i_e = 1:N_epochs\n\n    Flux.train!(model, train, opt_state) do m, t, x\n        result = m((t,x))\n        loss(result, x)\n    end \n\n    global l = mean(valid) do v\n        loss( model(v), v[2] )\n    end\n    \n    if i_e % 30 == 0\n        global η /= 2\n        Flux.adjust!(opt_state, η)\n    end\n\nend\n\nl","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"model(valid[1])","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"valid[1][2]","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"fig1, ax1 = plot_nde(sol, model, train, ndata=150)\n\nsave(\"nde1.png\", fig1); nothing # hide","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"(Image: )","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"This error, and the following solution, was discovered in the development of this project. For a dynamical system, we expect the error to compound exponentially. Hence it would seem beneficial to ensure that the model stays as close to the true solution at the beginning of the trajectory as possible. To encourage this, we add an exponential weight factor:","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"L(mathbfx mathbfhatx mathbfp) = sum_i=1^n beta^i cdot  mathbfx(t_i) - mathbfhatx(t_i mathbfp)  ^2","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"where beta in (01). While optimizing parameters, beta can be optimized as well. During testing, an optimal value of beta = 099 was observed. ","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"nn = Chain(\n    Dense(3 => N_weights),\n\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n    SkipConnection(Dense(N_weights => N_weights, swish), +),\n\n    Dense(N_weights => 3)\n)\n\np, re_nn = Flux.destructure(nn)\n#p = p |> gpu\nneural_ode(u, p, t) = re_nn(p)(u)\nneural_ode(u) = neural_ode(u, p, 0f0)\n\nneural_ode_prob = ODEProblem(neural_ode, #=CuArray(x0)=#x0, tspan, p)\nmodel = ChaoticNDE(neural_ode_prob, alg=RK4(), gpu=false#=true=#, sensealg=InterpolatingAdjoint())","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"β = 0.99f0\nfunction loss(x, y, β)\n    n = size(x, 2)\n    βs = β .^ (1:n)\n    sum( abs2, (x - y) .* βs' )\nend\n\nl = mean(valid) do v\n    loss( model(v), v[2], 1f0 )\nend\n\nθ = 1f-4\nη = 1f-3\nopt = Flux.OptimiserChain(Flux.WeightDecay(θ), Flux.RMSProp(η))\nopt_state = Flux.setup(opt, model) \n\nN_epochs = 30\nfor i_e = 1:N_epochs\n\n    Flux.train!(model, train, opt_state) do m, t, x\n        result = m((t,x))\n        loss(result, x, β)\n    end \n\n    global l = mean(valid) do v\n        loss( model(v), v[2], 1f0 )\n    end\n    \n    if i_e % 30 == 0\n        global η /= 2\n        Flux.adjust!(opt_state, η)\n    end\n\nend\n\nl","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"model(valid[1])","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"valid[1][2]","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"fig2, ax2 = plot_nde(sol, model, train, ndata=150)\n\nsave(\"nde2.png\", fig2); nothing # hide","category":"page"},{"location":"model/","page":"Model","title":"Model","text":"(Image: )","category":"page"},{"location":"#Neural-ODE-Project","page":"Home","title":"Neural ODE Project","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the Final Project for the lecture \"Modelling and Machine Learning of Dynamical Systems in Julia\" at the Technical University Munich. The julia package format is used to easily transfer environments, but does not contain actual code itself. The report is written as the documentation, though equivalent Jupyter notebooks and scripts are available in the package repo as well. ","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[1] Ricky T. Q. Chen et al. Neural Ordinary Differential Equations. 2019. arXiv: 1806.07366 [cs.LG].","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[2] Chetvorno. Chua’s circuit. url: https://en.wikipedia.org/wiki/File:Chua%5C%27scircuitwithChuadiode.svg.","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[3] Chetvorno. The current–voltage characteristic of the Chua diode. url:https://en.wikipedia.org/wiki/File:Chuadiodecharacteristic_curve.svg.","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[4] Leon Ong Chua. “The Genesis of Chua’s circuit”. In: 1992.","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[5] Gary Froyland and Kathrin Padberg-Gehle. “Almost-invariant sets and invariant manifolds — Connecting probabilistic and geometric descriptions of coherent structures in flows”. In: Physica D Nonlinear Phenomena (Aug. 2009), pp. 1507–1523. doi: 10.1016/j.physd.2009.03.002.","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[6] Kaiming He et al. Deep Residual Learning for Image Recognition. 2015. arXiv: 1512.03385 [cs.CV]","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[7] April Herwig. “GAIO.jl: Set-oriented Methods for Approximating Invariant Objects, and their Implementation in Julia”. Thesis. 2022. url: https://github.com/April-Hannah-Lena/schoolwork/blob/2eada059678d91bad8a813c3e6b657a1ac72e86f/Thesis/main.pdf","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[8] Michael P. Kennedy. “Robust op amp realization of Chua’s circuit”. In: Proc. First Experimental Chaos Conf. (1992), pp. 340–351.","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[9] Michael P. Kennedy. “Three Steps to Chaos - Part 1: Evolution”. In: IEEE Transactions on Circuits and Systems - 1: Fundamental Theory and Applications 40.10 (1993).","category":"page"},{"location":"refs/","page":"References","title":"References","text":"[10] Leonid P. Shilnikov. “Chua’s Circuit: Rigorous Results and Future Problems”. In: International Journal on Bifurcations and Chaos 4.3 (1994), pp. 489–519.","category":"page"}]
}
